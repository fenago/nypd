{"metadata":{"orig_nbformat":4,"colab":{"name":"Copy of Chapter 13 - Unbalanced Data sets","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Note - Oversampling can not be done in binder.  There is not enough compute resources","metadata":{}},{"cell_type":"code","source":"# Loading the necessary library files\nimport pandas as pd","metadata":{"id":"h6UINJEPtGQJ","colab_type":"code","colab":{},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Loading data from the drive\n\n# Please change the filename as per the location where the file is stored\nfilename = 'https://raw.githubusercontent.com/fenago/nypd/main/data/NYPD_SQF.csv'\n# Loading the data using pandas\n\nnypdData = pd.read_csv(filename,sep=\",\")\nnypdData.head()","metadata":{"id":"B4bxRZg_tqnp","colab_type":"code","outputId":"8f916c9e-31cc-437b-eb3c-90746a1ff121","colab":{"base_uri":"https://localhost:8080/","height":224},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   year  pct sex race  stopped  arrested  frisked  searched  summoned  \\\n0  2003  1.0   F    A      1.0       0.0      1.0       1.0       0.0   \n1  2003  1.0   F    B     11.0       2.0      2.0       0.0       0.0   \n2  2003  1.0   F    I      0.0       0.0      0.0       0.0       0.0   \n3  2003  1.0   F    P      1.0       0.0      0.0       0.0       0.0   \n4  2003  1.0   F    Q      6.0       1.0      1.0       0.0       0.0   \n\n   contrabn  ...  cs_bulge  rf_vcrim  rf_othsw  rf_attir  rf_vcact  rf_rfcmp  \\\n0       0.0  ...       0.0       0.0       1.0       0.0       0.0       0.0   \n1       0.0  ...       0.0       2.0       2.0       0.0       0.0       0.0   \n2       0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n3       0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n4       0.0  ...       0.0       0.0       0.0       1.0       0.0       0.0   \n\n   rf_verbl  rf_knowl  rf_furtv  rf_bulge  \n0       0.0       0.0       0.0       0.0  \n1       0.0       0.0       0.0       0.0  \n2       0.0       0.0       0.0       0.0  \n3       0.0       0.0       0.0       0.0  \n4       0.0       0.0       0.0       0.0  \n\n[5 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>pct</th>\n      <th>sex</th>\n      <th>race</th>\n      <th>stopped</th>\n      <th>arrested</th>\n      <th>frisked</th>\n      <th>searched</th>\n      <th>summoned</th>\n      <th>contrabn</th>\n      <th>...</th>\n      <th>cs_bulge</th>\n      <th>rf_vcrim</th>\n      <th>rf_othsw</th>\n      <th>rf_attir</th>\n      <th>rf_vcact</th>\n      <th>rf_rfcmp</th>\n      <th>rf_verbl</th>\n      <th>rf_knowl</th>\n      <th>rf_furtv</th>\n      <th>rf_bulge</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2003</td>\n      <td>1.0</td>\n      <td>F</td>\n      <td>A</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2003</td>\n      <td>1.0</td>\n      <td>F</td>\n      <td>B</td>\n      <td>11.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2003</td>\n      <td>1.0</td>\n      <td>F</td>\n      <td>I</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2003</td>\n      <td>1.0</td>\n      <td>F</td>\n      <td>P</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2003</td>\n      <td>1.0</td>\n      <td>F</td>\n      <td>Q</td>\n      <td>6.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature engineering steps**\n\nLet us now do some feature engineering to the data. First we will scale the numerical data and then convert the ordinal data to \ndummy data","metadata":{"id":"3tLgOzokaNiH","colab_type":"text"}},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\n\nrob_scaler = RobustScaler()\n\n# Converting each of the columns to scaled version\n\nnypdData['stoppedScaled'] = rob_scaler.fit_transform(nypdData['stopped'].values.reshape(-1,1))\nnypdData['arrestedScaled'] = rob_scaler.fit_transform(nypdData['arrested'].values.reshape(-1,1))\nnypdData['friskedScaled'] = rob_scaler.fit_transform(nypdData['frisked'].values.reshape(-1,1))\nnypdData['searchedScaled'] = rob_scaler.fit_transform(nypdData['searched'].values.reshape(-1,1))\nnypdData['summonedScaled'] = rob_scaler.fit_transform(nypdData['summoned'].values.reshape(-1,1))\nnypdData['contrabnScaled'] = rob_scaler.fit_transform(nypdData['contrabn'].values.reshape(-1,1))\nnypdData['weapnfndScaled'] = rob_scaler.fit_transform(nypdData['weapnfnd'].values.reshape(-1,1))\nnypdData['pf_weapnScaled'] = rob_scaler.fit_transform(nypdData['pf_weapn'].values.reshape(-1,1))\nnypdData['pf_hcuffScaled'] = rob_scaler.fit_transform(nypdData['pf_hcuff'].values.reshape(-1,1))\nnypdData['cs_objcsScaled'] = rob_scaler.fit_transform(nypdData['cs_objcs'].values.reshape(-1,1))\nnypdData['cs_descrScaled'] = rob_scaler.fit_transform(nypdData['cs_descr'].values.reshape(-1,1))\nnypdData['cs_casngScaled'] = rob_scaler.fit_transform(nypdData['cs_casng'].values.reshape(-1,1))\nnypdData['cs_lkoutScaled'] = rob_scaler.fit_transform(nypdData['cs_lkout'].values.reshape(-1,1))\nnypdData['cs_clothScaled'] = rob_scaler.fit_transform(nypdData['cs_cloth'].values.reshape(-1,1))\nnypdData['cs_drgtrScaled'] = rob_scaler.fit_transform(nypdData['cs_drgtr'].values.reshape(-1,1))\nnypdData['cs_furtvScaled'] = rob_scaler.fit_transform(nypdData['cs_furtv'].values.reshape(-1,1))\nnypdData['cs_vcrimScaled'] = rob_scaler.fit_transform(nypdData['cs_vcrim'].values.reshape(-1,1))\nnypdData['cs_bulgeScaled'] = rob_scaler.fit_transform(nypdData['cs_bulge'].values.reshape(-1,1))\nnypdData['rf_vcrimScaled'] = rob_scaler.fit_transform(nypdData['rf_vcrim'].values.reshape(-1,1))\nnypdData['rf_othswScaled'] = rob_scaler.fit_transform(nypdData['rf_othsw'].values.reshape(-1,1))\nnypdData['rf_attirScaled'] = rob_scaler.fit_transform(nypdData['rf_attir'].values.reshape(-1,1))\nnypdData['rf_vcactScaled'] = rob_scaler.fit_transform(nypdData['rf_vcact'].values.reshape(-1,1))\nnypdData['rf_rfcmpScaled'] = rob_scaler.fit_transform(nypdData['rf_rfcmp'].values.reshape(-1,1))\nnypdData['rf_verblScaled'] = rob_scaler.fit_transform(nypdData['rf_verbl'].values.reshape(-1,1))\nnypdData['rf_knowlScaled'] = rob_scaler.fit_transform(nypdData['rf_knowl'].values.reshape(-1,1))\nnypdData['rf_furtvScaled'] = rob_scaler.fit_transform(nypdData['rf_furtv'].values.reshape(-1,1))\nnypdData['rf_bulgeScaled'] = rob_scaler.fit_transform(nypdData['rf_bulge'].values.reshape(-1,1))\n\n\n# Dropping the original columns\n\nnypdData.drop(['year','stopped','arrested','frisked','searched','summoned','contrabn','weapnfnd','pf_weapn','pf_hcuff','cs_objcs','cs_descr','cs_casng','cs_lkout','cs_cloth','cs_drgtr','cs_furtv','cs_vcrim','cs_bulge','rf_othsw','rf_attir','rf_vcact','rf_rfcmp','rf_verbl','rf_knowl','rf_furtv','rf_bulge'],\n              axis=1, inplace=True)\n\n# Print the head of the data\n\nnypdData.head()","metadata":{"id":"LjcB4SaWiKp0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":224},"outputId":"c58cff7f-1b1d-4613-d9cd-e69f669f247c","trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   pct sex race  rf_vcrim  stoppedScaled  arrestedScaled  friskedScaled  \\\n0  1.0   F    A       0.0      -0.189573         -0.1875      -0.136364   \n1  1.0   F    B       2.0      -0.142180         -0.0625      -0.125000   \n2  1.0   F    I       0.0      -0.194313         -0.1875      -0.147727   \n3  1.0   F    P       0.0      -0.189573         -0.1875      -0.147727   \n4  1.0   F    Q       0.0      -0.165877         -0.1250      -0.136364   \n\n   searchedScaled  summonedScaled  contrabnScaled  ...  cs_bulgeScaled  \\\n0       -0.105263       -0.166667            -0.2  ...       -0.090909   \n1       -0.157895       -0.166667            -0.2  ...       -0.090909   \n2       -0.157895       -0.166667            -0.2  ...       -0.090909   \n3       -0.157895       -0.166667            -0.2  ...       -0.090909   \n4       -0.157895       -0.166667            -0.2  ...       -0.090909   \n\n   rf_vcrimScaled  rf_othswScaled  rf_attirScaled  rf_vcactScaled  \\\n0       -0.111111       -0.066667            -0.1       -0.083333   \n1        0.000000        0.000000            -0.1       -0.083333   \n2       -0.111111       -0.133333            -0.1       -0.083333   \n3       -0.111111       -0.133333            -0.1       -0.083333   \n4       -0.111111       -0.133333             0.0       -0.083333   \n\n   rf_rfcmpScaled  rf_verblScaled  rf_knowlScaled  rf_furtvScaled  \\\n0       -0.133333             0.0             0.0       -0.142857   \n1       -0.133333             0.0             0.0       -0.142857   \n2       -0.133333             0.0             0.0       -0.142857   \n3       -0.133333             0.0             0.0       -0.142857   \n4       -0.133333             0.0             0.0       -0.142857   \n\n   rf_bulgeScaled  \n0       -0.090909  \n1       -0.090909  \n2       -0.090909  \n3       -0.090909  \n4       -0.090909  \n\n[5 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pct</th>\n      <th>sex</th>\n      <th>race</th>\n      <th>rf_vcrim</th>\n      <th>stoppedScaled</th>\n      <th>arrestedScaled</th>\n      <th>friskedScaled</th>\n      <th>searchedScaled</th>\n      <th>summonedScaled</th>\n      <th>contrabnScaled</th>\n      <th>...</th>\n      <th>cs_bulgeScaled</th>\n      <th>rf_vcrimScaled</th>\n      <th>rf_othswScaled</th>\n      <th>rf_attirScaled</th>\n      <th>rf_vcactScaled</th>\n      <th>rf_rfcmpScaled</th>\n      <th>rf_verblScaled</th>\n      <th>rf_knowlScaled</th>\n      <th>rf_furtvScaled</th>\n      <th>rf_bulgeScaled</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>F</td>\n      <td>A</td>\n      <td>0.0</td>\n      <td>-0.189573</td>\n      <td>-0.1875</td>\n      <td>-0.136364</td>\n      <td>-0.105263</td>\n      <td>-0.166667</td>\n      <td>-0.2</td>\n      <td>...</td>\n      <td>-0.090909</td>\n      <td>-0.111111</td>\n      <td>-0.066667</td>\n      <td>-0.1</td>\n      <td>-0.083333</td>\n      <td>-0.133333</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.142857</td>\n      <td>-0.090909</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>F</td>\n      <td>B</td>\n      <td>2.0</td>\n      <td>-0.142180</td>\n      <td>-0.0625</td>\n      <td>-0.125000</td>\n      <td>-0.157895</td>\n      <td>-0.166667</td>\n      <td>-0.2</td>\n      <td>...</td>\n      <td>-0.090909</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.1</td>\n      <td>-0.083333</td>\n      <td>-0.133333</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.142857</td>\n      <td>-0.090909</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>F</td>\n      <td>I</td>\n      <td>0.0</td>\n      <td>-0.194313</td>\n      <td>-0.1875</td>\n      <td>-0.147727</td>\n      <td>-0.157895</td>\n      <td>-0.166667</td>\n      <td>-0.2</td>\n      <td>...</td>\n      <td>-0.090909</td>\n      <td>-0.111111</td>\n      <td>-0.133333</td>\n      <td>-0.1</td>\n      <td>-0.083333</td>\n      <td>-0.133333</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.142857</td>\n      <td>-0.090909</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>F</td>\n      <td>P</td>\n      <td>0.0</td>\n      <td>-0.189573</td>\n      <td>-0.1875</td>\n      <td>-0.147727</td>\n      <td>-0.157895</td>\n      <td>-0.166667</td>\n      <td>-0.2</td>\n      <td>...</td>\n      <td>-0.090909</td>\n      <td>-0.111111</td>\n      <td>-0.133333</td>\n      <td>-0.1</td>\n      <td>-0.083333</td>\n      <td>-0.133333</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.142857</td>\n      <td>-0.090909</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>F</td>\n      <td>Q</td>\n      <td>0.0</td>\n      <td>-0.165877</td>\n      <td>-0.1250</td>\n      <td>-0.136364</td>\n      <td>-0.157895</td>\n      <td>-0.166667</td>\n      <td>-0.2</td>\n      <td>...</td>\n      <td>-0.090909</td>\n      <td>-0.111111</td>\n      <td>-0.133333</td>\n      <td>0.0</td>\n      <td>-0.083333</td>\n      <td>-0.133333</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.142857</td>\n      <td>-0.090909</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Converting all the categorical variables to dummy variables\nnypdCat = pd.get_dummies(nypdData[['pct','sex','race']])","metadata":{"id":"8glxMnxxN8NB","colab_type":"code","colab":{},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Seperating the numerical data\nnypdNum = nypdData[['stoppedScaled','arrestedScaled','friskedScaled','searchedScaled','summonedScaled','contrabnScaled','weapnfndScaled','pf_weapnScaled','pf_hcuffScaled','cs_objcsScaled','cs_descrScaled','cs_casngScaled','cs_lkoutScaled','cs_clothScaled','cs_drgtrScaled','cs_furtvScaled','cs_vcrimScaled','cs_bulgeScaled','rf_othswScaled','rf_attirScaled','rf_vcactScaled','rf_rfcmpScaled','rf_verblScaled','rf_knowlScaled','rf_furtvScaled','rf_bulgeScaled']]\nnypdNum.shape","metadata":{"id":"Ge2LlC_1ePzX","colab_type":"code","outputId":"ef455def-cf0c-4a9c-eb1c-6583a7015579","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(12012, 26)"},"metadata":{}}]},{"cell_type":"code","source":"# Merging with the original data frame\n# Preparing the X variables\nX = pd.concat([nypdCat, nypdNum], axis=1)\nprint(X.shape)\n# Preparing the Y variable - this is what we will be predicting\n# Choose one of the categorical variables\nY = nypdData['race']\nprint(Y.shape)\nX.head()","metadata":{"id":"EsT4u_jDQ7Sv","colab_type":"code","outputId":"aaff137c-2e31-4555-f1b3-d1ff3750650f","colab":{"base_uri":"https://localhost:8080/","height":275},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"(12012, 35)\n(12012,)\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   pct  sex_F  sex_M  race_A  race_B  race_I  race_P  race_Q  race_W  \\\n0  1.0      1      0       1       0       0       0       0       0   \n1  1.0      1      0       0       1       0       0       0       0   \n2  1.0      1      0       0       0       1       0       0       0   \n3  1.0      1      0       0       0       0       1       0       0   \n4  1.0      1      0       0       0       0       0       1       0   \n\n   stoppedScaled  ...  cs_vcrimScaled  cs_bulgeScaled  rf_othswScaled  \\\n0      -0.189573  ...       -0.153846       -0.090909       -0.066667   \n1      -0.142180  ...       -0.153846       -0.090909        0.000000   \n2      -0.194313  ...       -0.153846       -0.090909       -0.133333   \n3      -0.189573  ...       -0.153846       -0.090909       -0.133333   \n4      -0.165877  ...       -0.153846       -0.090909       -0.133333   \n\n   rf_attirScaled  rf_vcactScaled  rf_rfcmpScaled  rf_verblScaled  \\\n0            -0.1       -0.083333       -0.133333             0.0   \n1            -0.1       -0.083333       -0.133333             0.0   \n2            -0.1       -0.083333       -0.133333             0.0   \n3            -0.1       -0.083333       -0.133333             0.0   \n4             0.0       -0.083333       -0.133333             0.0   \n\n   rf_knowlScaled  rf_furtvScaled  rf_bulgeScaled  \n0             0.0       -0.142857       -0.090909  \n1             0.0       -0.142857       -0.090909  \n2             0.0       -0.142857       -0.090909  \n3             0.0       -0.142857       -0.090909  \n4             0.0       -0.142857       -0.090909  \n\n[5 rows x 35 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pct</th>\n      <th>sex_F</th>\n      <th>sex_M</th>\n      <th>race_A</th>\n      <th>race_B</th>\n      <th>race_I</th>\n      <th>race_P</th>\n      <th>race_Q</th>\n      <th>race_W</th>\n      <th>stoppedScaled</th>\n      <th>...</th>\n      <th>cs_vcrimScaled</th>\n      <th>cs_bulgeScaled</th>\n      <th>rf_othswScaled</th>\n      <th>rf_attirScaled</th>\n      <th>rf_vcactScaled</th>\n      <th>rf_rfcmpScaled</th>\n      <th>rf_verblScaled</th>\n      <th>rf_knowlScaled</th>\n      <th>rf_furtvScaled</th>\n      <th>rf_bulgeScaled</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.189573</td>\n      <td>...</td>\n      <td>-0.153846</td>\n      <td>-0.090909</td>\n      <td>-0.066667</td>\n      <td>-0.1</td>\n      <td>-0.083333</td>\n      <td>-0.133333</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.142857</td>\n      <td>-0.090909</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.142180</td>\n      <td>...</td>\n      <td>-0.153846</td>\n      <td>-0.090909</td>\n      <td>0.000000</td>\n      <td>-0.1</td>\n      <td>-0.083333</td>\n      <td>-0.133333</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.142857</td>\n      <td>-0.090909</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.194313</td>\n      <td>...</td>\n      <td>-0.153846</td>\n      <td>-0.090909</td>\n      <td>-0.133333</td>\n      <td>-0.1</td>\n      <td>-0.083333</td>\n      <td>-0.133333</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.142857</td>\n      <td>-0.090909</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.189573</td>\n      <td>...</td>\n      <td>-0.153846</td>\n      <td>-0.090909</td>\n      <td>-0.133333</td>\n      <td>-0.1</td>\n      <td>-0.083333</td>\n      <td>-0.133333</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.142857</td>\n      <td>-0.090909</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-0.165877</td>\n      <td>...</td>\n      <td>-0.153846</td>\n      <td>-0.090909</td>\n      <td>-0.133333</td>\n      <td>0.0</td>\n      <td>-0.083333</td>\n      <td>-0.133333</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.142857</td>\n      <td>-0.090909</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 35 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n# Splitting the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=123)\n# Defining the LogisticRegression function\nnypdModel = LogisticRegression()\nnypdModel.fit(X_train, y_train)","metadata":{"id":"h8DjrmI53bEt","colab_type":"code","outputId":"9652aa5c-8518-4b1d-aa50-d9708232d3e9","colab":{"base_uri":"https://localhost:8080/","height":156},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"LogisticRegression()"},"metadata":{}}]},{"cell_type":"code","source":"pred = nypdModel.predict(X_test)\nprint('Accuracy of Logistic regression model prediction on test set: {:.2f}'.format(nypdModel.score(X_test, y_test)))\n\n# Confusion Matrix for the model\nfrom sklearn.metrics import confusion_matrix\nconfusionMatrix = confusion_matrix(y_test, pred)\nprint(confusionMatrix)\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, pred))","metadata":{"id":"5JWHRmRTTloL","colab_type":"code","outputId":"6f2297bc-531f-47e9-e1c9-c390a634a5ec","colab":{"base_uri":"https://localhost:8080/","height":221},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Accuracy of Logistic regression model prediction on test set: 0.97\n[[610   2   0   0   3   0]\n [  0 584   3   0  13   5]\n [ 15   0 578   0   1   2]\n [  0   4   0 571   5   3]\n [  3  11   8   2 560   5]\n [  1   4   0   2  12 597]]\n              precision    recall  f1-score   support\n\n           A       0.97      0.99      0.98       615\n           B       0.97      0.97      0.97       605\n           I       0.98      0.97      0.98       596\n           P       0.99      0.98      0.99       583\n           Q       0.94      0.95      0.95       589\n           W       0.98      0.97      0.97       616\n\n    accuracy                           0.97      3604\n   macro avg       0.97      0.97      0.97      3604\nweighted avg       0.97      0.97      0.97      3604\n\n","output_type":"stream"}]},{"cell_type":"code","source":"print('Percentage of positive class :',(y_train[y_train=='yes'].value_counts()/len(y_train) ) * 100)\nprint('Percentage of negative class :',(y_train[y_train=='no'].value_counts()/len(y_train) ) * 100)","metadata":{"id":"6SNINkDUhqe8","colab_type":"code","outputId":"d639890f-a63a-4f58-fa85-c9a73c084be3","colab":{"base_uri":"https://localhost:8080/","height":85},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compare Algorithms\nfrom sklearn.metrics import roc_auc_score\nfrom time import time\nfrom sklearn.metrics import explained_variance_score,mean_absolute_error,r2_score\nfrom pandas import read_csv\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='liblinear')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    start = time()\n    kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n    model.fit(X_train, y_train)\n    train_time = time() - start\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    predict_time = time()-start \n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    #y_pred = model.predict_proba(X_train)[:, 1]\n    #auc = roc_auc_score(y_train, y_pred)\n    print(msg)\n    print(\"Score for each of the 10 K-fold tests: \",cv_results)\n    print(model)\n    print(\"\\tTraining time: %0.3fs\" % train_time)\n    print(\"\\tPrediction time: %0.3fs\" % predict_time)\n    #y_pred = model.predict(X_test)\n    #print(\"\\tExplained variance:\", explained_variance_score(y_test, y_pred))\n    print()\n    \n    \n    \n# boxplot algorithm comparison\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Undersampling Method.**\n\nIn the random undersampling method, we down sample the majority class to the same amount as the minority class to make the data set balanced. Let us see how we can achieve that\n\nIn this method we first identify the count of  the  minority cases and then undersample the majority cases to be the same as minority cases. \n\n\n","metadata":{"id":"WUDWDskEfUux","colab_type":"text"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Splitting the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=123)\n","metadata":{"id":"DLad9gfcKCeo","colab_type":"code","colab":{}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let us first join the train_x and train_y for ease of operation\n\ntrainData = pd.concat([X_train,y_train],axis=1)\ntrainData.head()","metadata":{"id":"-Q_AHv77lTwk","colab_type":"code","outputId":"ffdff3f2-191c-46d0-b996-656ca81a263e","colab":{"base_uri":"https://localhost:8080/","height":241}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finding the indexes of the sample data set where the propensity is 'yes'\nind = trainData[trainData['y']=='yes'].index\nprint(len(ind))\n\n# Seperate the minority classes\nminData = trainData.loc[ind]\nprint(minData.shape)\n\n# Finding indexes of majority class\nind1 = trainData[trainData['y']=='no'].index\nprint(len(ind1))\n# Seperating the majority class\nmajData = trainData.loc[ind1]\nprint(majData.shape)\nmajData.head()","metadata":{"id":"ykfsvEdgfTtt","colab_type":"code","outputId":"b7efc9db-0e5a-4a4d-b73f-0de5605f1030","colab":{"base_uri":"https://localhost:8080/","height":309}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Take a random sample equal to length of the minority class to make the data set balanced\n\nmajSample = majData.sample(n=len(ind),random_state = 123)\nprint(majSample.shape)\nmajSample.head()\n\n","metadata":{"id":"f2APbvt3ovEP","colab_type":"code","outputId":"06598bb1-580f-414e-9bf9-304e02d0fe12","colab":{"base_uri":"https://localhost:8080/","height":258}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatinating both data sets and then shuffling the data set\n\nbalData = pd.concat([minData,majSample],axis = 0)\nprint('balanced data set shape',balData.shape)\n\n# Shuffling the data set\n\nfrom sklearn.utils import shuffle\n\nbalData = shuffle(balData)\nbalData.head()","metadata":{"id":"kPgQS0ZEp8qV","colab_type":"code","outputId":"339b0fe3-8e2b-4e45-a1e1-572c53afd697","colab":{"base_uri":"https://localhost:8080/","height":258}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making the new X_train and y_train\n\nX_trainNew = balData.iloc[:,0:51]\nX_trainNew.head()\n\ny_trainNew = balData['y']\ny_trainNew.head()","metadata":{"id":"oKqn7nO8pDVX","colab_type":"code","outputId":"000d829f-1334-47fe-c0c9-a2d6b4d49c87","colab":{"base_uri":"https://localhost:8080/","height":119}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Defining the LogisticRegression function\nbankModel1 = LogisticRegression()\nbankModel1.fit(X_trainNew, y_trainNew)\n\n# Predicting on the test\npred = bankModel1.predict(X_test)\nprint('Accuracy of Logisticr regression model prediction on test set for balanced data set: {:.2f}'.format(bankModel1.score(X_test, y_test)))\n\n","metadata":{"id":"9nXONaFanHR4","colab_type":"code","outputId":"3084e814-7e54-46ed-999c-2c68048af599","colab":{"base_uri":"https://localhost:8080/","height":88}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion Matrix for the model\nfrom sklearn.metrics import confusion_matrix\nconfusionMatrix = confusion_matrix(y_test, pred)\nprint(confusionMatrix)","metadata":{"id":"-BmM1E58xNEh","colab_type":"code","outputId":"aecf1a8e-f0f2-4695-b23b-977a759a7584","colab":{"base_uri":"https://localhost:8080/","height":51}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion Matrix for the model\nfrom sklearn.metrics import confusion_matrix\nconfusionMatrix = confusion_matrix(y_test, pred)\nprint(confusionMatrix)\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, pred))","metadata":{"id":"xCidAiJbxjuI","colab_type":"code","outputId":"20f9e97c-7b9c-4748-c12f-4756af1611b3","colab":{"base_uri":"https://localhost:8080/","height":204}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Random Over Sampling**\n\nLet us now try the over sampling method and find what effect it has on the results","metadata":{"id":"tVageK142kfy","colab_type":"text"}},{"cell_type":"code","source":"!pip install smote-variants","metadata":{"id":"4NrPQWkA9Eyf","colab_type":"code","colab":{}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the data into train and test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n\nprint(\"Before OverSampling count of yes: {}\".format(sum(y_train=='yes')))\nprint(\"Before OverSampling count of no: {} \\n\".format(sum(y_train=='no')))","metadata":{"id":"WPTjlvKdYHd2","colab_type":"code","outputId":"ecb38d3a-40f2-487c-ab23-4d828149b68f","colab":{"base_uri":"https://localhost:8080/","height":68}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import smote_variants as sv\nimport numpy as np\n\n# Instantiating the SMOTE class\noversampler= sv.SMOTE()\n\n# Creating new training set\n\nX_train_us, y_train_us = oversampler.sample(np.array(X_train), np.array(y_train))\n\n# Shape after oversampling\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_train_us.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_us.shape))\n\nprint(\"After OverSampling, counts of label 'Yes': {}\".format(sum(y_train_us=='yes')))\nprint(\"After OverSampling, counts of label 'no': {}\".format(sum(y_train_us=='no')))\n","metadata":{"id":"dFVuB-2K83QE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"64cccac7-64ab-40e8-8357-0f2cfd1e1bd3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training the model with Logistic regression model\n\n# Defining the LogisticRegression function\n\nbankModel2 = LogisticRegression()\n\nbankModel2.fit(X_train_us, y_train_us)\n\n# Predicting on the test set\npred = bankModel2.predict(X_test)\n\n# Printing accuracy \nprint('Accuracy of Logistic regression model prediction on test set for Smote balanced data set: {:.2f}'.format(bankModel2.score(X_test, y_test)))\n\n# Confusion Matrix for the model\n\nfrom sklearn.metrics import confusion_matrix\nconfusionMatrix = confusion_matrix(y_test, pred)\nprint(confusionMatrix)\n\n# Classification report for the model\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, pred))\n\n","metadata":{"id":"fd3PNkJE9-WU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":275},"outputId":"fa51847b-3346-4028-b21c-741dcd867272"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Activity 1**\n\nImplementing MSMOTE","metadata":{"id":"XCpFjq2R_72o","colab_type":"text"}},{"cell_type":"code","source":"# Splitting the data into train and test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n\nprint(\"Before OverSampling count of yes: {}\".format(sum(y_train=='yes')))\nprint(\"Before OverSampling count of no: {} \\n\".format(sum(y_train=='no')))","metadata":{"id":"zkKa5ZHmADAI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"8942ddc4-0d0d-4f2f-e6c5-94035a6a516d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import smote_variants as sv\nimport numpy as np\n# Instantiating the SMOTE class\noversampler= sv.MSMOTE()\n# Creating new training sts\nX_train_us, y_train_us = oversampler.sample(np.array(X_train), np.array(y_train))\n\n# Shape after oversampling\nprint('After OverSampling, the shape of train_X: {}'.format(X_train_us.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_us.shape))\n\nprint(\"After OverSampling, counts of label 'Yes': {}\".format(sum(y_train_us=='yes')))\nprint(\"After OverSampling, counts of label 'no': {}\".format(sum(y_train_us=='no')))","metadata":{"id":"xOr_xeTnsXlD","colab_type":"code","outputId":"7ac1cc57-6956-41a8-e483-f44f820f594c","colab":{"base_uri":"https://localhost:8080/","height":139}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting model\n\n# Training the model with Logistic regression model\n\n# Defining the LogisticRegression function\nbankModel2 = LogisticRegression()\nbankModel2.fit(X_train_us, y_train_us)\n\n# Predicting on the test\npred = bankModel2.predict(X_test)\nprint('Accuracy of Logistic regression model prediction on test set for Smote balanced data set: {:.2f}'.format(bankModel2.score(X_test, y_test)))\n\n# Confusion Matrix for the model\nfrom sklearn.metrics import confusion_matrix\nconfusionMatrix = confusion_matrix(y_test, pred)\nprint(confusionMatrix)\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, pred))","metadata":{"id":"c-_FD6bGmu4S","colab_type":"code","outputId":"6570402c-5e1a-4ed6-8ff5-4d9f4c59cf0d","colab":{"base_uri":"https://localhost:8080/","height":275}},"execution_count":null,"outputs":[]}]}